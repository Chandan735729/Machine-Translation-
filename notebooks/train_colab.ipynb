{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üåê English-Assamese Translation Model Training (Updated)\n",
    "\n",
    "This notebook fine-tunes Meta's NLLB model for English-Assamese translation using the **Helsinki-NLP/opus-100** dataset.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)**\n",
    "2. **Run all cells in order**\n",
    "3. **Monitor training progress**\n",
    "4. **Download the trained model**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "print(\"Installing required packages...\")\n",
    "!pip install -q torch transformers datasets accelerate sentencepiece\n",
    "!pip install -q pandas numpy tqdm sacrebleu\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 2. Clone Repository and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your actual repo URL)\n",
    "!git clone https://github.com/Chandan735729/Machine-Translation-.git\n",
    "%cd Machine-Translation-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep"
   },
   "source": [
    "## 3. Dataset Loading with Helsinki-NLP/opus-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"üîÑ Loading Helsinki-NLP/opus-100 dataset for English-Assamese...\")\n",
    "\n",
    "# Load the opus-100 dataset with English-Assamese pair\n",
    "try:\n",
    "    # Load opus-100 dataset with as-en language pair (reverse direction)\n",
    "    dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"as-en\")\n",
    "    print(\"‚úÖ Successfully loaded Helsinki-NLP/opus-100 dataset\")\n",
    "    print(f\"Available splits: {list(dataset.keys())}\")\n",
    "    print(f\"Train samples: {len(dataset['train'])}\")\n",
    "    print(f\"Test samples: {len(dataset['test'])}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nüìù Sample data:\")\n",
    "    for i in range(3):\n",
    "        sample = dataset['train'][i]\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  English: {sample['translation']['en']}\")\n",
    "        print(f\"  Assamese: {sample['translation']['as']}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading opus-100: {e}\")\n",
    "    print(\"üîÑ Creating fallback dataset...\")\n",
    "    \n",
    "    # Fallback to sample dataset with proper structure\n",
    "    sample_data_train = [\n",
    "        {'en': 'Hello, how are you?', 'as': '‡¶®‡¶Æ‡¶∏‡ßç‡¶ï‡¶æ‡ß∞, ‡¶Ü‡¶™‡ßÅ‡¶®‡¶ø ‡¶ï‡ßá‡¶®‡ßá ‡¶Ü‡¶õ‡ßá?'},\n",
    "        {'en': 'Thank you very much.', 'as': '‡¶¨‡¶π‡ßÅ‡¶§ ‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶‡•§'},\n",
    "        {'en': 'Good morning.', 'as': '‡¶∂‡ßÅ‡¶≠ ‡ß∞‡¶æ‡¶§‡¶ø‡¶™‡ßÅ‡ß±‡¶æ‡•§'},\n",
    "        {'en': 'How can I help you?', 'as': '‡¶Æ‡¶á ‡¶Ü‡¶™‡ßã‡¶®‡¶æ‡¶ï ‡¶ï‡ßá‡¶®‡ßá‡¶ï‡ßà ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º ‡¶ï‡ß∞‡¶ø‡¶¨ ‡¶™‡¶æ‡ß∞‡ßã?'},\n",
    "        {'en': 'Education is very important.', 'as': '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ ‡¶Ö‡¶§‡¶ø ‡¶ó‡ßÅ‡ß∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡ß∞‡ßç‡¶£‡•§'},\n",
    "        {'en': 'Health is wealth.', 'as': '‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø‡¶á ‡¶∏‡¶Æ‡ßç‡¶™‡¶¶‡•§'},\n",
    "        {'en': 'Water is essential for life.', 'as': '‡¶ú‡ßÄ‡ß±‡¶®‡ß∞ ‡¶¨‡¶æ‡¶¨‡ßá ‡¶™‡¶æ‡¶®‡ßÄ ‡¶Ö‡¶™‡ß∞‡¶ø‡¶π‡¶æ‡ß∞‡ßç‡¶Ø‡•§'},\n",
    "        {'en': 'Children need proper nutrition.', 'as': '‡¶∂‡¶ø‡¶∂‡ßÅ‡¶∏‡¶ï‡¶≤‡ß∞ ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶™‡ßÅ‡¶∑‡ßç‡¶ü‡¶ø‡ß∞ ‡¶™‡ßç‡ß∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®‡•§'},\n",
    "        {'en': 'Clean environment is important.', 'as': '‡¶™‡ß∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡ß∞ ‡¶™‡ß∞‡¶ø‡ß±‡ßá‡¶∂ ‡¶ó‡ßÅ‡ß∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡ß∞‡ßç‡¶£‡•§'},\n",
    "        {'en': 'Technology helps development.', 'as': '‡¶™‡ßç‡ß∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶®‡ßç‡¶®‡¶Ø‡¶º‡¶®‡¶§ ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º ‡¶ï‡ß∞‡ßá‡•§'},\n",
    "        {'en': 'Women empowerment is crucial.', 'as': '‡¶Æ‡¶π‡¶ø‡¶≤‡¶æ ‡¶∏‡ß±‡¶≤‡ßÄ‡¶ï‡ß∞‡¶£ ‡¶Ö‡¶§‡¶ø ‡¶ó‡ßÅ‡ß∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡ß∞‡ßç‡¶£‡•§'},\n",
    "        {'en': 'Agriculture feeds the nation.', 'as': '‡¶ï‡ßÉ‡¶∑‡¶ø‡¶Ø‡¶º‡ßá ‡¶¶‡ßá‡¶∂‡¶ï ‡¶ñ‡ßÅ‡ß±‡¶æ‡¶Ø‡¶º‡•§'},\n",
    "        {'en': 'Peace brings prosperity.', 'as': '‡¶∂‡¶æ‡¶®‡ßç‡¶§‡¶ø‡¶Ø‡¶º‡ßá ‡¶∏‡¶Æ‡ßÉ‡¶¶‡ßç‡¶ß‡¶ø ‡¶Ü‡¶®‡ßá‡•§'},\n",
    "        {'en': 'Knowledge is power.', 'as': '‡¶ú‡ßç‡¶û‡¶æ‡¶®‡ßá‡¶á ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡•§'},\n",
    "        {'en': 'Unity in diversity.', 'as': '‡¶¨‡ßà‡¶ö‡¶ø‡¶§‡ßç‡ß∞‡ßç‡¶Ø‡ß∞ ‡¶Æ‡¶æ‡¶ú‡¶§ ‡¶ê‡¶ï‡ßç‡¶Ø‡•§'},\n",
    "        {'en': 'Hard work pays off.', 'as': '‡¶ï‡¶†‡ßã‡ß∞ ‡¶™‡ß∞‡¶ø‡¶∂‡ßç‡ß∞‡¶Æ‡ß∞ ‡¶´‡¶≤ ‡¶™‡ßã‡ß±‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§'},\n",
    "        {'en': 'Time is precious.', 'as': '‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶Ö‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡•§'},\n",
    "        {'en': 'Respect your elders.', 'as': '‡¶¨‡¶Ø‡¶º‡ßã‡¶ú‡ßç‡¶Ø‡ßá‡¶∑‡ßç‡¶†‡¶∏‡¶ï‡¶≤‡¶ï ‡¶∏‡¶®‡ßç‡¶Æ‡¶æ‡¶® ‡¶ï‡ß∞‡¶ï‡•§'},\n",
    "        {'en': 'Nature is beautiful.', 'as': '‡¶™‡ßç‡ß∞‡¶ï‡ßÉ‡¶§‡¶ø ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡ß∞‡•§'},\n",
    "        {'en': 'Love your country.', 'as': '‡¶®‡¶ø‡¶ú‡ß∞ ‡¶¶‡ßá‡¶∂‡¶ï ‡¶≠‡¶æ‡¶≤ ‡¶™‡¶æ‡¶ì‡¶ï‡•§'}\n",
    "    ]\n",
    "    \n",
    "    sample_data_val = [\n",
    "        {'en': 'Good evening.', 'as': '‡¶∂‡ßÅ‡¶≠ ‡¶∏‡¶®‡ßç‡¶ß‡¶ø‡¶Ø‡¶º‡¶æ‡•§'},\n",
    "        {'en': 'See you tomorrow.', 'as': '‡¶ï‡¶æ‡¶á‡¶≤‡ßà ‡¶≤‡¶ó ‡¶™‡¶æ‡¶Æ‡•§'},\n",
    "        {'en': 'Take care of yourself.', 'as': '‡¶®‡¶ø‡¶ú‡ß∞ ‡¶Ø‡¶§‡ßç‡¶® ‡¶≤‡¶ì‡¶ï‡•§'},\n",
    "        {'en': 'Have a nice day.', 'as': '‡¶¶‡¶ø‡¶®‡¶ü‡ßã ‡¶≠‡¶æ‡¶≤ ‡¶ï‡¶ü‡¶æ‡¶ì‡¶ï‡•§'},\n",
    "        {'en': 'Welcome to Assam.', 'as': '‡¶Ö‡¶∏‡¶Æ‡¶≤‡ßà ‡¶∏‡ßç‡¶¨‡¶æ‡¶ó‡¶§‡¶Æ‡•§'}\n",
    "    ]\n",
    "    \n",
    "    from datasets import Dataset, DatasetDict\n",
    "    dataset = DatasetDict({\n",
    "        'train': Dataset.from_list(sample_data_train),\n",
    "        'validation': Dataset.from_list(sample_data_val)\n",
    "    })\n",
    "    print(f\"‚úÖ Created fallback dataset with {len(dataset['train'])} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_exploration"
   },
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore_dataset"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Data Preprocessing\n",
    "print(\"üîÑ Setting up tokenizer and preprocessing...\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Language codes for NLLB\n",
    "source_lang = \"eng_Latn\"  # English\n",
    "target_lang = \"asm_Beng\"  # Assamese\n",
    "\n",
    "# Set source and target languages for the tokenizer\n",
    "tokenizer.src_lang = source_lang\n",
    "tokenizer.tgt_lang = target_lang\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the dataset for training\"\"\"\n",
    "    # Extract source and target texts\n",
    "    if 'translation' in examples:\n",
    "        # Handle opus-100 format\n",
    "        inputs = [ex['en'] for ex in examples['translation']]  # English as input\n",
    "        targets = [ex['as'] for ex in examples['translation']]  # Assamese as target\n",
    "    else:\n",
    "        # Handle fallback format (direct en/as columns)\n",
    "        inputs = examples['en']\n",
    "        targets = examples['as']\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"üìä Preprocessing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocessing completed!\")\n",
    "print(f\"Train samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(tokenized_dataset.get('validation', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "from train import TranslationTrainer\n",
    "import os\n",
    "\n",
    "# Initialize trainer\n",
    "print(\"ü§ñ Initializing translation trainer...\")\n",
    "trainer_obj = TranslationTrainer()\n",
    "\n",
    "# Load processed data\n",
    "print(\"üìÇ Loading processed dataset...\")\n",
    "dataset = trainer_obj.load_processed_data()\n",
    "\n",
    "print(f\"\\nüìã Training Configuration:\")\n",
    "print(f\"  Model: facebook/nllb-200-distilled-600M\")\n",
    "print(f\"  Dataset: ai4bharat/sangraha (English-Assamese)\")\n",
    "print(f\"  Train samples: {len(dataset['train'])}\")\n",
    "print(f\"  Validation samples: {len(dataset.get('validation', []))}\")\n",
    "print(f\"  Device: {trainer_obj.device}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ Starting model training...\")\n",
    "print(\"This may take 30-60 minutes depending on your GPU and dataset size.\")\n",
    "\n",
    "trainer, model_path = trainer_obj.train_model(dataset)\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"üìÅ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"üìä Evaluating model performance...\")\n",
    "eval_results = trainer_obj.evaluate_model(trainer, dataset)\n",
    "\n",
    "print(f\"\\nüìà Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_translation"
   },
   "source": [
    "## 7. Test Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test"
   },
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "from translate import EnglishToAssameseTranslator\n",
    "\n",
    "# Initialize translator with trained model\n",
    "print(\"üîÑ Loading trained model for testing...\")\n",
    "translator = EnglishToAssameseTranslator(model_path)\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"Community health workers are the backbone of our medical system.\",\n",
    "    \"Education is the key to development.\",\n",
    "    \"Clean water is essential for good health.\",\n",
    "    \"Vaccination protects children from diseases.\",\n",
    "    \"Women's empowerment leads to stronger communities.\",\n",
    "    \"Hello, how are you?\",\n",
    "    \"Thank you for your help.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing translations with trained model:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"\\n{i}. English: {sentence}\")\n",
    "    translation = translator.translate(sentence)\n",
    "    print(f\"   Assamese: {translation}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ Translation testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 8. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# Create a zip file of the trained model\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "model_dir = \"models/nllb-finetuned-en-to-asm-final\"\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    print(\"üì¶ Creating model archive...\")\n",
    "    \n",
    "    # Create zip file\n",
    "    shutil.make_archive(\"trained_model_sangraha\", 'zip', model_dir)\n",
    "    \n",
    "    print(\"‚úÖ Model archived as 'trained_model_sangraha.zip'\")\n",
    "    print(\"üì• Download it from the Files panel on the left\")\n",
    "    \n",
    "    # Show file size\n",
    "    size_mb = os.path.getsize(\"trained_model_sangraha.zip\") / (1024 * 1024)\n",
    "    print(f\"üìä Archive size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Also save training info\n",
    "    import json\n",
    "    training_info = {\n",
    "        \"dataset\": \"ai4bharat/sangraha\",\n",
    "        \"base_model\": \"facebook/nllb-200-distilled-600M\",\n",
    "        \"language_pair\": \"English-Assamese\",\n",
    "        \"training_date\": str(pd.Timestamp.now()),\n",
    "        \"model_path\": model_dir\n",
    "    }\n",
    "    \n",
    "    with open(\"training_info.json\", \"w\") as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "    \n",
    "    print(\"üìÑ Training info saved to 'training_info.json'\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Model directory not found. Training may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_to_drive"
   },
   "source": [
    "## 9. Optional: Upload to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_upload"
   },
   "outputs": [],
   "source": [
    "# Optional: Mount Google Drive and upload model\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"üîó Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy model to Drive\n",
    "drive_path = \"/content/drive/MyDrive/translation_models/\"\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "if os.path.exists(\"trained_model_sangraha.zip\"):\n",
    "    shutil.copy(\"trained_model_sangraha.zip\", f\"{drive_path}trained_model_sangraha.zip\")\n",
    "    shutil.copy(\"training_info.json\", f\"{drive_path}training_info.json\")\n",
    "    print(f\"‚úÖ Model and info uploaded to Google Drive: {drive_path}\")\n",
    "else:\n",
    "    print(\"‚ùå Model archive not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### What's New:\n",
    "- ‚úÖ **Updated Dataset**: Now using `ai4bharat/sangraha` instead of PMIndia\n",
    "- ‚úÖ **Fixed Tokenization**: Resolved the deprecated `as_target_tokenizer` warning\n",
    "- ‚úÖ **Better Error Handling**: Multiple fallback options for dataset loading\n",
    "- ‚úÖ **Flexible Column Mapping**: Handles different dataset column formats\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download** the trained model (`trained_model_sangraha.zip`)\n",
    "2. **Extract** it to your local project's `models/` directory\n",
    "3. **Update** the model path in your local `translate.py` if needed\n",
    "4. **Test** the model locally using the FastAPI backend\n",
    "\n",
    "### Model Usage:\n",
    "```python\n",
    "from translate import EnglishToAssameseTranslator\n",
    "\n",
    "translator = EnglishToAssameseTranslator(\"models/nllb-finetuned-en-to-asm-final\")\n",
    "result = translator.translate(\"Hello, how are you?\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Deployment:\n",
    "- Place the model in your project directory\n",
    "- Run the FastAPI server: `python run_server.py`\n",
    "- Access the web interface at `http://localhost:8000`\n",
    "\n",
    "---\n",
    "**Happy Translating with Sangraha Dataset! üåê**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
