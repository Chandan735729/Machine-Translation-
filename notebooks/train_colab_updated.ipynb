{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 🌐 English-Assamese Translation Model Training (Updated)\n",
    "\n",
    "This notebook fine-tunes Meta's NLLB model for English-Assamese translation using the **ai4bharat/sangraha** dataset.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Runtime → Change runtime type → GPU (T4 recommended)**\n",
    "2. **Run all cells in order**\n",
    "3. **Monitor training progress**\n",
    "4. **Download the trained model**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets accelerate sentencepiece\n",
    "!pip install -q pandas numpy tqdm\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 2. Clone Repository and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your actual repo URL)\n",
    "!git clone https://github.com/your-username/Machine-Translation-.git\n",
    "%cd Machine-Translation-\n",
    "\n",
    "# List files to verify\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep"
   },
   "source": [
    "## 3. Data Preparation with Sangraha Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "# Import and run data preparation\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from data_preparation import DataPreparator\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize data preparator with sangraha dataset\n",
    "print(\"🔄 Initializing data preparation with ai4bharat/sangraha dataset...\")\n",
    "preparator = DataPreparator()\n",
    "\n",
    "# Load and prepare dataset\n",
    "print(\"📥 Loading sangraha dataset...\")\n",
    "raw_dataset = preparator.load_dataset(\"ai4bharat/sangraha\")\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"📊 Dataset structure:\")\n",
    "print(f\"Available splits: {list(raw_dataset.keys())}\")\n",
    "if 'train' in raw_dataset:\n",
    "    print(f\"Train columns: {raw_dataset['train'].column_names}\")\n",
    "    print(f\"Train size: {len(raw_dataset['train'])}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\n📝 Sample data:\")\n",
    "    for i in range(min(3, len(raw_dataset['train']))):\n",
    "        sample = raw_dataset['train'][i]\n",
    "        print(f\"Sample {i+1}: {sample}\")\n",
    "\n",
    "print(\"\\n⚙️ Processing dataset...\")\n",
    "processed_dataset = preparator.prepare_datasets(raw_dataset)\n",
    "\n",
    "# Save processed data\n",
    "print(\"💾 Saving processed data...\")\n",
    "preparator.save_processed_data(processed_dataset)\n",
    "\n",
    "# Print statistics\n",
    "stats = preparator.get_data_stats(processed_dataset)\n",
    "print(f\"\\n📊 Dataset Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_exploration"
   },
   "source": [
    "## 4. Dataset Exploration (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore_dataset"
   },
   "outputs": [],
   "source": [
    "# Explore the dataset structure in more detail\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"🔍 Exploring ai4bharat/sangraha dataset...\")\n",
    "\n",
    "# Try to load and inspect the dataset\n",
    "try:\n",
    "    # Check available configurations\n",
    "    from datasets import get_dataset_config_names\n",
    "    configs = get_dataset_config_names(\"ai4bharat/sangraha\")\n",
    "    print(f\"Available configurations: {configs}\")\n",
    "    \n",
    "    # Try different configurations for English-Assamese\n",
    "    possible_configs = ['eng-asm', 'en-as', 'english-assamese']\n",
    "    \n",
    "    for config in possible_configs:\n",
    "        if config in configs:\n",
    "            print(f\"\\n✅ Found configuration: {config}\")\n",
    "            dataset = load_dataset(\"ai4bharat/sangraha\", config)\n",
    "            print(f\"Splits: {list(dataset.keys())}\")\n",
    "            if 'train' in dataset:\n",
    "                print(f\"Columns: {dataset['train'].column_names}\")\n",
    "                print(f\"Sample: {dataset['train'][0]}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"❌ No suitable English-Assamese configuration found\")\n",
    "        print(\"Available configs:\", configs)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error exploring dataset: {e}\")\n",
    "    print(\"Will use fallback sample dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "from train import TranslationTrainer\n",
    "import os\n",
    "\n",
    "# Initialize trainer\n",
    "print(\"🤖 Initializing translation trainer...\")\n",
    "trainer_obj = TranslationTrainer()\n",
    "\n",
    "# Load processed data\n",
    "print(\"📂 Loading processed dataset...\")\n",
    "dataset = trainer_obj.load_processed_data()\n",
    "\n",
    "print(f\"\\n📋 Training Configuration:\")\n",
    "print(f\"  Model: facebook/nllb-200-distilled-600M\")\n",
    "print(f\"  Dataset: ai4bharat/sangraha (English-Assamese)\")\n",
    "print(f\"  Train samples: {len(dataset['train'])}\")\n",
    "print(f\"  Validation samples: {len(dataset.get('validation', []))}\")\n",
    "print(f\"  Device: {trainer_obj.device}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n🚀 Starting model training...\")\n",
    "print(\"This may take 30-60 minutes depending on your GPU and dataset size.\")\n",
    "\n",
    "trainer, model_path = trainer_obj.train_model(dataset)\n",
    "\n",
    "print(f\"\\n✅ Training completed!\")\n",
    "print(f\"📁 Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"📊 Evaluating model performance...\")\n",
    "eval_results = trainer_obj.evaluate_model(trainer, dataset)\n",
    "\n",
    "print(f\"\\n📈 Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_translation"
   },
   "source": [
    "## 7. Test Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test"
   },
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "from translate import EnglishToAssameseTranslator\n",
    "\n",
    "# Initialize translator with trained model\n",
    "print(\"🔄 Loading trained model for testing...\")\n",
    "translator = EnglishToAssameseTranslator(model_path)\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"Community health workers are the backbone of our medical system.\",\n",
    "    \"Education is the key to development.\",\n",
    "    \"Clean water is essential for good health.\",\n",
    "    \"Vaccination protects children from diseases.\",\n",
    "    \"Women's empowerment leads to stronger communities.\",\n",
    "    \"Hello, how are you?\",\n",
    "    \"Thank you for your help.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 Testing translations with trained model:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"\\n{i}. English: {sentence}\")\n",
    "    translation = translator.translate(sentence)\n",
    "    print(f\"   Assamese: {translation}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n✅ Translation testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 8. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# Create a zip file of the trained model\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "model_dir = \"models/nllb-finetuned-en-to-asm-final\"\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    print(\"📦 Creating model archive...\")\n",
    "    \n",
    "    # Create zip file\n",
    "    shutil.make_archive(\"trained_model_sangraha\", 'zip', model_dir)\n",
    "    \n",
    "    print(\"✅ Model archived as 'trained_model_sangraha.zip'\")\n",
    "    print(\"📥 Download it from the Files panel on the left\")\n",
    "    \n",
    "    # Show file size\n",
    "    size_mb = os.path.getsize(\"trained_model_sangraha.zip\") / (1024 * 1024)\n",
    "    print(f\"📊 Archive size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Also save training info\n",
    "    import json\n",
    "    training_info = {\n",
    "        \"dataset\": \"ai4bharat/sangraha\",\n",
    "        \"base_model\": \"facebook/nllb-200-distilled-600M\",\n",
    "        \"language_pair\": \"English-Assamese\",\n",
    "        \"training_date\": str(pd.Timestamp.now()),\n",
    "        \"model_path\": model_dir\n",
    "    }\n",
    "    \n",
    "    with open(\"training_info.json\", \"w\") as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "    \n",
    "    print(\"📄 Training info saved to 'training_info.json'\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Model directory not found. Training may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_to_drive"
   },
   "source": [
    "## 9. Optional: Upload to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_upload"
   },
   "outputs": [],
   "source": [
    "# Optional: Mount Google Drive and upload model\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"🔗 Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy model to Drive\n",
    "drive_path = \"/content/drive/MyDrive/translation_models/\"\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "if os.path.exists(\"trained_model_sangraha.zip\"):\n",
    "    shutil.copy(\"trained_model_sangraha.zip\", f\"{drive_path}trained_model_sangraha.zip\")\n",
    "    shutil.copy(\"training_info.json\", f\"{drive_path}training_info.json\")\n",
    "    print(f\"✅ Model and info uploaded to Google Drive: {drive_path}\")\n",
    "else:\n",
    "    print(\"❌ Model archive not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 🎉 Training Complete!\n",
    "\n",
    "### What's New:\n",
    "- ✅ **Updated Dataset**: Now using `ai4bharat/sangraha` instead of PMIndia\n",
    "- ✅ **Fixed Tokenization**: Resolved the deprecated `as_target_tokenizer` warning\n",
    "- ✅ **Better Error Handling**: Multiple fallback options for dataset loading\n",
    "- ✅ **Flexible Column Mapping**: Handles different dataset column formats\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download** the trained model (`trained_model_sangraha.zip`)\n",
    "2. **Extract** it to your local project's `models/` directory\n",
    "3. **Update** the model path in your local `translate.py` if needed\n",
    "4. **Test** the model locally using the FastAPI backend\n",
    "\n",
    "### Model Usage:\n",
    "```python\n",
    "from translate import EnglishToAssameseTranslator\n",
    "\n",
    "translator = EnglishToAssameseTranslator(\"models/nllb-finetuned-en-to-asm-final\")\n",
    "result = translator.translate(\"Hello, how are you?\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Deployment:\n",
    "- Place the model in your project directory\n",
    "- Run the FastAPI server: `python run_server.py`\n",
    "- Access the web interface at `http://localhost:8000`\n",
    "\n",
    "---\n",
    "**Happy Translating with Sangraha Dataset! 🌐**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
